<h1><b>Understanding Prosper Loan Data Set</b></h1>
<p><small><em>Uir?? Caiado. April 7, 2015</em></small></p>
------------------
```{r, include=FALSE, cache=TRUE}
install.packages('ggthemes',repos="http://cran.rstudio.com/")


library("plyr")
library("tidyr")
library("dplyr")
library(ggplot2)
library("ggthemes")
library(gridExtra)
library(reshape2)

theme_set(theme_minimal(24))
#theme_set(theme_fivethirtyeight(24))


#rm(list = setdiff(ls(), lsf.str()))

s_fname <- "~/git/ProsperLoan_EDA/prosperLoanData.csv"
d <- read.csv(s_fname)
```

<h2>Abstract</h2>

I explore a data set about P2P Loans.  Key goals of the study are to understand ..., and ... .

<h2>Introduction</h2>

I have to tell the truth. Initially, the number of variables intimidated me. There are 113,937 rows in this data set and 81 columns (variables). It seems to come from Prosper website, a <em><a href="http://en.wikipedia.org/wiki/Peer-to-peer_lending">peer-to-peer lending</a></em> marketplace. I have already seen a brazilian website trying to do something similar in my country, but our "SEC" shut them off. Just financial companies can lend money here.

According to <a href = "http://en.wikipedia.org/wiki/Prosper_Marketplace#Loan_performance_prior_to_July_2009">wikipedia</a>, the SEC imposed that Prosper had to cease their operations  due to violations on <a href="http://www.investopedia.com/terms/s/securitiesact1933.asp">a piece of legistion</a>  that ensures transparency and try to avoid fraudulent activities in the american securities markets. The website took from November 2008 to July 2009 to resume their activities. 

Bellow you can see the effect of this time closed:

```{r, echo=FALSE, cache=TRUE, fig.width=14, fig.height=9}
#creating new variables
d$LoanOriginationDate <- as.Date(d$LoanOriginationDate,format = "%Y-%m-%d")
d$LoanOriginationDate.year=format(d$LoanOriginationDate, "%Y")

ggplot(aes(x = LoanOriginationDate.year), data = d) +
  geom_histogram() + 
  ggtitle('Number of Loans by Year')
```

There are 6 variables in the data set that seems to be created to comply those rules. They are just valid to loans originated after 2009 and, in general, they are all about how much interest have been payed and how risky each loan were. Looking at other variables, I can see that they are pretty diverse but they are all bout those two factors: risk and return.

There are variable explicit about risk measurements (what was the classification when the loan was took, the scores from Prosper), about how much was payed and earned, meta data of the loan (the size in money and duration, when and where it was originated), how was the life of the borrower when he took the loan(employment status, how much money he earns, how in debt he is, what he intend to do with the money), how each loan was founded and so on.

I am interested explore this binomial risk/return. I would like to know if the risk measurments are really effective.

<h2>Analysis</h2>

<h3>Univariate Section</h3>

In Brazil, where I live, I would expect the total loans rise at some months of the year. Usually at the begining and the end of the year, when people travel, pay taxes and buy gifts. Let's start looking at the amount of money borrowed by month in United States.


```{r, echo=FALSE, cache=TRUE, fig.width=14, fig.height=9}
#creating new variables
l_months <- c('Jan','Feb','Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep','Oct',
              'Nov', 'Dec')
d$LoanOriginationDate.month=format(d$LoanOriginationDate, "%b")
d$LoanOriginationDate.month <- factor(d$LoanOriginationDate.month, l_months)


ggplot(aes(x = LoanOriginationDate.month, y = LoanOriginalAmount), data = d) +
  geom_bar(stat="identity") +
  ggtitle('Money Borrowed By Month')
```

As expected, the amount of loans rises at first and last months of the year. Well, 
if there are more money borrowed, probably after some month would have more money
delinquent. Let's see

```{r, echo=FALSE, cache=TRUE, fig.width=14, fig.height=9}
#creating new variables
l_months <- c('Jan','Feb','Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep','Oct',
              'Nov', 'Dec')
d$LoanOriginationDate.month=format(d$LoanOriginationDate, "%b")
d$LoanOriginationDate.month <- factor(d$LoanOriginationDate.month, l_months)


ggplot(aes(x = LoanOriginationDate.month, y = LoanOriginalAmount), data = d) +
  geom_bar(stat="identity") +
  ggtitle('Money Borrowed By Month')
```


And how about the the distribution of the interest payed ?





-The most...
-Talk about the median...
-Quartis...
-The mediad and The max...

-What are the main features?
-What other feature that can help?
-Unusual distributions...

-histograms




```{r, cache=TRUE, fig.width=14, fig.height=9}

s_fname <- "~/git/ProsperLoan_EDA/prosperLoanData.csv"

d <- read.csv(s_fname)

#creating new variables
l_months <- c('Jan','Feb','Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep','Oct',
              'Nov', 'Dec')

d$LoanOriginationDate <- as.Date(d$LoanOriginationDate,format = "%Y-%m-%d")
d$LoanOriginationDate.year=format(d$LoanOriginationDate, "%Y")
d$LoanOriginationDate.month=format(d$LoanOriginationDate, "%b")
d$LoanOriginationDate.month <- factor(d$LoanOriginationDate.month, l_months)


p1 <- ggplot(aes(x = LoanOriginationDate.year), data = d) +
  geom_histogram() +
  ggtitle('Year')

p2 <- ggplot(aes(x = LoanOriginationDate.year, y = LoanOriginalAmount), data = d) +
  geom_bar(stat="identity") +
  ggtitle('Year')


p3 <- ggplot(aes(x = LoanOriginationDate.month), data = d) +
  geom_histogram() +
  ggtitle('Month')

p4 <- ggplot(aes(x = LoanOriginationDate.month, y = LoanOriginalAmount), data = d) +
  geom_bar(stat="identity") +
  ggtitle('Month')


grid.arrange(p1, p3, ncol=1)

grid.arrange(p2, p4, ncol=1)


```





-Statistics 'by' Gender

```{r Statistics \'by\' Gender}
table(pf$gender)


by(pf$friend_count, pf$gender, summary)

```



### Add a Scaling Layer
Notes:

```{r Add a Scaling Layer}
install.packages("gridExtra")#sthash.TtPNxnxv.dpuf
library(gridExtra)

summary(pf$friend_count)

summary(log10(pf$friend_count + 1))


summary(sqrt(pf$friend_count))


p1 <- qplot(x = friend_count, data = pf, 
           xlab = 'Number of friends',
           ylab = 'Number of users in sample',           
           binwidth = 10) +
  scale_x_continuous(limits = c(0, 1000),
                     breaks = seq(0, 1000, 50)) 

p2 <- qplot(x = friend_count, data = pf,
           xlab = 'log(Number of friends)',
           ylab = 'Number of users in sample') +
  scale_x_log10(limits = c(1, 1000),
                     breaks = seq(1, 1000, 50)) 


p3 <- qplot(x = friend_count, data = pf, 
           xlab = 'sqrt(Number of friends)',
           ylab = 'Number of users in sample') +
  scale_x_sqrt(limits = c(0, 1000),
                     breaks = seq(0, 1000, 50)) 

grid.arrange(p1, p2, p3, ncol=3)


p1 <- ggplot(aes(x = friend_count), data = pf) + geom_histogram()

p2 <- p1 + scale_x_log10()

p3 <- p1 + scale_x_sqrt()



grid.arrange(p1, p2, p3, ncol=1)

```

***


### Frequency Polygons

```{r Frequency Polygons}
ggplot(aes(x = friend_count, y = ..count../sum(..count..)), data = subset(pf, !is.na(gender))) +
  geom_freqpoly(aes(color = gender)) + 
  scale_x_continuous(limits = c(0, 1000), breaks = seq(0, 1000, 50)) +
  xlab('Friend Count') + 
  ylab('Percentage of users with that friend count')

```

***

### Likes on the Web
Notes:

```{r Likes on the Web}

summary(pf$www_likes)


ggplot(aes(x = www_likes), data = subset(pf, !is.na(gender))) +
  geom_freqpoly(aes(color = gender)) + 
  scale_x_log10()

by(pf$www_likes, pf$gender, sum)
```


***

### Box Plots
Notes:

```{r Box Plots}
qplot(x = gender, y = friend_count,
      data = subset(pf, !is.na(gender)),
      geom = 'boxplot') 
```

#### Adjust the code to focus on users who have friend counts between 0 and 1000.

```{r}
qplot(x = gender, y = friend_count,
      data = subset(pf, !is.na(gender)),
      geom = 'boxplot') +
  scale_y_continuous(limits = c(0, 1000))
```

***

### Box Plots, Quartiles, and Friendships
Notes:

```{r Box Plots, Quartiles, and Friendships}
qplot(x = gender, y = friendships_initiated,
      data = subset(pf, !is.na(gender)),
      geom = 'boxplot') +
  scale_y_continuous(limits = c(0, 250))

by(pf$friendships_initiated, pf$gender, sum)

by(pf$friendships_initiated, pf$gender, summary)
```

#### On average, who initiated more friendships in our sample: men or women?
Response:
#### Write about some ways that you can verify your answer.
Response:
```{r Friend Requests by Gender}

```

Response:

***

### Getting Logical
Notes:

```{r Getting Logical}
summary(pf$mobile_likes)

summary(pf$mobile_likes > 0)

pf$mobile_check_in <- NA
pf$mobile_check_in <- ifelse(pf$mobile_likes>0, 1, 0)
pf$mobile_check_in <- factor(pf$mobile_check_in )
my_summ <- summary(pf$mobile_check_in)
my_summ[2]/(my_summ[1]+my_summ[2])

sum(pf$mobile_check_in==1)/length(pf$mobile_check_in)
```

Response:

***

### Analyzing One Variable
Reflection:

-Observing the values that your variable take, like missing values, distribution and outliers;
-Boxplot, frequency polygons and histograns are the basic and most important tools to undertand individual variable;
-We use transformarmations like log, square and booleans to uncover hidden patterns in the variables;



Scales and Multiple Histograms

``` {r,echo=FALSE}

qplot(x = price, data = diamonds) + facet_wrap(~cut, scale="free_y")
?facet_wrap

```



###Interquartile Range - IQR

```{r,echo=FALSE}

#find out the best and worst color
?diamonds

#Price range for the middle 50% of diamonds with color D (best)
x <- subset(diamonds, color == 'D')$price 
IQR(x) 
summary(x)

#Price range for the middle 50% of diamonds with color J (worst)
x <- subset(diamonds, color == 'J')$price 
IQR(x) 
summary(x)



```









<h3>Bivariate Section</h3>

-Relationship observed
-What are the strongest relationship found


### Scatterplot Matrix
Notes:
```{r Looking at Sample of Households}

install.packages('GGally')

library(GGally)

set.seed(1836)
pf_subset <- pf[,c(2:15)]
names(pf_subset)
ggpairs(pf_subset[sample.int(nrow(pf_subset), 1000), ])


```





### Alpha and Jitter
Notes:


It is curious to notice that the distribution to friends_count and friend 
initiated are pretty the same. Maybe the most of users are relatively new on the
website.

```{r Alpha and Jitter}
# Examine the relationship between friendships_initiated (y) and age (x)
# using the ggplot syntax.

# We recommend creating a basic scatter plot first to see what the distribution 
#looks like and then adjusting it by adding one layer at a time.

# What are your observations about your final plot?

# Remember to make adjustments to the breaks of the x-axis and to use apply 
# alpha and jitter.
names(pf)

p1 <- ggplot(aes(x = age, y = friend_count), data = pf) + 
  geom_point(alpha = 1/20, position = position_jitter( h = 0)) + 
  coord_trans(y = "sqrt") +
  xlim(13, 90)

p2 <- ggplot(aes(x = age, y = friendships_initiated), data = pf) + 
  geom_point(alpha = 1/20, position = position_jitter( h = 0)) + 
  coord_trans(y = "sqrt") +
  xlim(13, 90)


grid.arrange(p1, p2, ncol=1)


```




### Conditional Means
Notes:

```{r Conditional Means}
#grouping data
age_groups <- group_by(pf, age)
pf.fc_by_age <- summarise(age_groups,
          friend_count_mean = mean(friend_count),
          fries_count_median = median(friend_count),
          n = n()
          )
head(pf.fc_by_age)
#as my dataframe is not arranged...
pf.fc_by_age <- arrange(pf.fc_by_age, age)
head(pf.fc_by_age)


#doing the same

pf.fc_by_age <- pf %>%#channing function
  group_by(age) %>%
  summarise(friend_count_mean = mean(friend_count),
          fries_count_median = median(friend_count),
          n = n()) %>%
          arrange(age)
head(pf.fc_by_age)

```

### Overlaying Summaries with Raw Data
Notes:

- People with more than one tawsand friends are quite rare (90% are bellow this value)
- 25 to 60 are far bellow 500;

```{r Overlaying Summaries with Raw Data}
ggplot(aes(x = age, y = friend_count), data = pf) +
  coord_cartesian(xlim = c(13, 90), 
                  ylim = c(0,2000)) +
  geom_point(alpha = 0.05,
            position = position_jitter(h=0),
            color = 'orange') + 
  geom_line(stat = 'summary', fun.y = mean) +
  geom_line(stat = 'summary', fun.y = quantile, prob = .1,
            linetype = 2, color = "blue") + 
  geom_line(stat = 'summary', fun.y = quantile, prob = .9,
            linetype = 2, color = "blue")



```


### Strong Correlations
Notes:

```{r Strong Correlations}
ggplot(aes(x = www_likes_received, y = likes_received), data = pf) +
  geom_point() +
  xlim (0, quantile(pf$www_likes_received, 0.95)) +
  ylim(0,quantile(pf$likes_received, 0.95)) +  
  geom_smooth(method = 'lm', color =  'red')
```

What's the correlation betwen the two variables? Include the top 5% of values for the variable in the calculation and round to 3 decimal places.

```{r Correlation Calcuation}
with(pf, cor.test(www_likes_received, likes_received))
```



### Making Sense of Data
Notes:

...it is sinusoidal... actually it makes sense because the seasons.... but it was cool to se in the chart when scratched

```{r Making Sense of Data}

range(Mitchell$Month)

ggplot(aes(y = Temp, x = Month), data = Mitchell) +
  geom_point() +
  scale_x_discrete(breaks = seq(0, 203,12))


ggplot(aes(x=(Month%%12),y=Temp),data=Mitchell)+ 
  geom_point() 
```

### Noise in Conditional Means

```{r Noise in Conditional Means}
p1 <- ggplot(aes(x = age_with_months, y = friend_count_mean), 
             data = subset(pf.fc_by_age_months, age_with_months < 71)) + 
  geom_line() +
  geom_smooth()


p2 <- ggplot(aes(x = age, y = friend_count_mean), 
             data = subset(pf.fc_by_age, age < 71)) + 
  geom_line() +
  geom_smooth()


p3 <- ggplot(aes(x = round(age/5)*5, y = friend_count_mean), 
             data = subset(pf.fc_by_age, age < 71)) + 
  geom_line()

library(gridExtra)
grid.arrange(p3, p2,p1,ncol=1)

```


###  create a new data frame containing info on diamonds by clarity
Notes:

...


```{r, echo=FALSE}

#grouping data
diamonds_groups <- group_by(diamonds, clarity)
diamondsByClarity <- summarise(diamonds_groups,
          mean_price = mean(price),
          median_price = median(as.numeric(price)),
          min_price = min(price),
          max_price = max(price),
          n = n()
          )
head(diamondsByClarity)

#as my dataframe is not arranged...
pf.fc_by_age <- arrange(pf.fc_by_age, age)
head(pf.fc_by_age)

```

***



###  Create a bar plot of dplying data
Notes:


first...for some crazy reason the best color and best clarity (D and IF) are not the greatest price either.


We think something odd is going here. These trends seem to go against our intuition.

Mean price tends to decrease as clarity improves. The same can be said for color.

We encourage you to look into the mean price across cut.

```{r, echo=FALSE}


?diamonds

diamonds_by_clarity <- group_by(diamonds, clarity)
diamonds_mp_by_clarity <- summarise(diamonds_by_clarity, mean_price = mean(price))

diamonds_by_color <- group_by(diamonds, color)
diamonds_mp_by_color <- summarise(diamonds_by_color, mean_price = mean(price))

diamonds_by_cut <- group_by(diamonds, cut)
diamonds_mp_by_cut <- summarise(diamonds_by_cut, mean_price = mean(price))

str(diamonds_mp_by_clarity)

p1 <- ggplot(aes(x = clarity, y= mean_price), data = diamonds_mp_by_clarity) + 
  geom_bar(stat="identity")
  
p2 <- ggplot(aes(x = color, y= mean_price), data = diamonds_mp_by_color) + 
  geom_bar(stat="identity") 

p3 <- ggplot(aes(x =cut, y= mean_price), data = diamonds_mp_by_cut) + 
  geom_bar(stat="identity") 

grid.arrange(p1, p2, p3, ncol = 1)
```


### Analyzing Two Variables
Reflection:

-sccater plots and conditional means;
-the bennefits and limitations of use of correlation and how correlation can affect
your decision on including the variables in you final model (do not use variables
without monotonic behaviour...that influence each other);
-we learned how to make sense of our data adjusting our visualizations...we learned
how jitter and alpha reduce overplotting;






<h3>Multivariate Section</h3>

-Relationship observed
-Any surprise?




### Histograms Revisited
Notes:

One clue to the discritness of data is that the 3rd percentile is the same of the maximum value

```{r Histograms Revisited}

yo <- read.csv('yogurt.csv')
str(yo)

#change the id from an int to a factor
yo$id <- factor(yo$id)
str(yo)

ggplot(data = yo, aes(x = price)) +
  geom_histogram()

ggplot(data = yo, aes(x = price)) +
  geom_histogram(binwidth=10)


summary(yo$price)

unique(yo$price)

length(unique(yo$price))

```

```{r, echo=FALSE}

sample_data <- function(sample_size = 3000){
  index <- sample(nrow(d), sample_size)
  return(d[index, ]) 
}

head(sample_data())

ggplot(aes(x = color, y = price), data = sample_data(sample_size = 5000)) +
  scale_color_brewer(type = 'div') +
  geom_boxplot(outlier.size = 0, color = "blue") +
  geom_jitter(position=position_jitter(width=0.3), alpha=0.2, 
              color = "steelblue2") +
  facet_wrap (~clarity)
```




<h2>Final Plots and Summary</h2>

- plot1 and description
- plot2 and description
- plot3 and description


<h2>Reflection</h2>

bla

