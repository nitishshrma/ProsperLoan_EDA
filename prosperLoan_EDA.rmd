<h1><b>Understanding Prosper Loan Data Set</b></h1>
<p><small><em>Uir?? Caiado. April 7, 2015</em></small></p>
------------------
```{r, include=FALSE, cache=TRUE}
#I nice reference: http://business.time.com/2012/11/15/taking-a-peek-at-peer-to-peer-lending/
install.packages('knitr',repos="http://cran.rstudio.com/")
install.packages("tidyr")


library("plyr")
library("tidyr")
library("dplyr")
library(ggplot2)
library("ggthemes")
library(gridExtra)
# library(reshape2)

theme_set(theme_minimal(17))
#theme_set(theme_fivethirtyeight(24))


#rm(list = setdiff(ls(), lsf.str()))

s_fname <- "~/git/ProsperLoan_EDA/prosperLoanData.csv"
d <- read.csv(s_fname)
```

<h2>Abstract</h2>

I explore a data set about P2P Loans.  Key goals of the study are to understand ..., and ... .

<h2>Introduction</h2>

I have to tell the truth. Initially, the number of variables intimidated me. There
are 113,937 rows in this data set and 81 columns (variables). It seems to come 
from Prosper website, a peer-to-peer <a 
href="http://en.wikipedia.org/wiki/Peer-to-peer_lending">(P2P)</a> lending marketplace. 
I have already seen a brazilian website trying to do something similar in my country,
but our "SEC" shut them off. Just financial companies can lend money here.

According to <a 
href = "http://en.wikipedia.org/wiki/Prosper_Marketplace#Loan_performance_prior_to_July_2009">wikipedia</a>,
the SEC imposed that Prosper had to cease their operations  due to violations on 
<a href="http://www.investopedia.com/terms/s/securitiesact1933.asp">a piece of legistion</a>
that ensures transparency and try to avoid fraudulent activities in the american 
securities markets. The website took from November 2008 to July 2009 to resume 
their activities. 

Bellow you can see the effect of this time closed:

```{r Number of Loans by Year, echo=FALSE, cache=TRUE, fig.width=11, fig.height=7}
#creating new variables
d$LoanOriginationDate <- as.Date(d$LoanOriginationDate,format = "%Y-%m-%d")
d$LoanOriginationDate.year=format(d$LoanOriginationDate, "%Y")

ggplot(aes(x = LoanOriginationDate.year), data = d) +
  geom_histogram() + 
  ggtitle('Number of Loans by Year')
```

There are 6 variables in the data set that seems to be created to comply those 
rules. They are just valid to loans originated after 2009 and, in general, they are
all about how much interest have been payed and how risky each loan were. Looking
at other variables, I can see that they are pretty diverse but they are all 
related to those two factors: risk and return.

There are variable explicit about risk measurements (what was the classification 
when the loan was took, the scores from Prosper), about how much was payed and earned,
meta data of the loan (the size in money and duration, when and where it was 
originated), about the profile of the borrower (employment status, his income, his
debts, what he intend to do with the money), how each loan was founded and so on.

As P2P lending is something new for me, I would like to understand how it works 
on United States and I will use this data base to acchive that.

<h2>Analysis</h2>

<h3>Univariate Section</h3>

In Brazil, where I live, I would expect the total loans rise at some months of 
the year. Usually at the begining and the end of the year, when people travel, 
pay taxes and buy gifts. Let's start looking at the amount of money borrowed by 
month in United States.


```{r Money Borrowed By Month, echo=FALSE, cache=TRUE, fig.width=11, fig.height=7}
#creating new variables
l_months <- c('Jan','Feb','Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep','Oct',
              'Nov', 'Dec')
d$LoanOriginationDate.month=format(d$LoanOriginationDate, "%b")
d$LoanOriginationDate.month <- factor(d$LoanOriginationDate.month, l_months)


ggplot(aes(x = LoanOriginationDate.month, y = LoanOriginalAmount), data = d) +
  geom_bar(stat="identity") +
  ggtitle('Money Borrowed By Month')
```

As expected, the amount of loans rises at first and last months of the year. Well, 
if there are more money borrowed on these periods, maybe the number of loans 
defaulted increases between them. To verify that, I have to use the ClosedDate 
variable and filter just the defaulted and chargedoff loans. Let's see:

```{r Amount Defaulted By Month, echo=FALSE, cache=TRUE, fig.width=11, fig.height=7}
#creating new variables
l_months <- c('Jan','Feb','Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep','Oct',
              'Nov', 'Dec')
d$ClosedDate <- as.Date(d$ClosedDate,format = "%Y-%m-%d")
d$ClosedDate.month=format(d$ClosedDate, "%b")
d$ClosedDate.month <- factor(d$ClosedDate.month, l_months)


ggplot(aes(x = ClosedDate.month, y = LoanOriginalAmount), data = subset(d, (LoanStatus == 'Defaulted') | (LoanStatus == 'Chargedoff'))) +
  geom_bar(stat="identity") +
  ggtitle('Amount Defaulted By Month')
```

Curriously the distribution of the amount borrowed by month looks like the 
distribution of amount defaulted/chargedoff by month. Well, I am not sure if this
filtered data is relevant, let me see how many data points there are in each status.
I will group all 'Past Due' buckets as the same buckets and show off the amount of
data on each status percentualy. As can be seem bellow, 11% of the data is related
to defaulted or chargedoff loans:

```{r stats of loan status, echo=FALSE, cache=TRUE, fig.width=14, fig.height=9}

rename_status <- function(s_status) {
  #return a string with renamed status passed
    if(substr(s_status, 0, 4)=='Past'){
      'Past Due Date'
    }else if(s_status == 'Chargedoff' | s_status =='Defaulted' ){
      'Chargedoff/Defaulted'
    }else{
      s_status
    }
}

#create a new variable and normalize the count of status already renamed
d$LoanStatus.renamed <- apply(d['LoanStatus'],1,rename_status)
my_summ <- table(d$LoanStatus.renamed, useNA = 'ifany')
round(my_summ/sum(my_summ)*100)/100

```

Thinking again, nobody would borrow money to pay back in the next couple of months,
at least not in Brazil. Let's see how many loans, percentually, were borrowed by 
duration:

```{r stats of terms, echo=FALSE, cache=TRUE, fig.width=14, fig.height=9}
my_summ <- table(d$Term)
round(my_summ/sum(my_summ)*100)/100
```

It explains why the distributions of Defaulted/Chargedoff loans and the amount 
borrowed by month looks like the same. As people take loans on regular durations,
12, 36 and 60 months, the loans will ever due to the same month in different years. 

How about the median of the loan? First, let's see the quartiles of the value of
each loan:

```{r summary of LoanOriginalAmount, echo=FALSE, cache=TRUE, fig.width=14, fig.height=9}
summary(d$LoanOriginalAmount)
```

Wow...it is a huge range. But it seems that the most of the loans are smaller, 
given that 50 % of the data set is up to $ 6500. Let's take a close look at the 
loan original amount split by duration. I had to convert the variable "Term" to
factor to make the chart bellow:

```{r Box Plots of Amount of Loan by Term}
d$Term.f <- as.factor(d$Term)

ggplot(aes(x = Term.f, y = LoanOriginalAmount), data = d) +
  scale_color_brewer(type = 'div') +
  geom_boxplot(outlier.size = 0, color = "black") +
  ggtitle('Amount of Loan by Term')
```

It seems that the loan for each duration is something that I should not plot 
together. The scale of each one is prety different.

Well, and how about the ineterest  rate? 

###TODO: talk about brazil...that the interest rate usually is homogenous

The distribution differs at each term?
First, let's the histogram of the interest paid:

```{r Number of Loans by Interest, echo=FALSE, cache=TRUE, fig.width=11, fig.height=7}
#creating new variables
summary(d$BorrowerRate)

ggplot(aes(x = BorrowerRate), data = d) +
  geom_histogram() + 
  ggtitle('Frequency of Interest Rate')
```

It seems that there is something curious on the tail. Let's reduce the bin width to 0.5%

```{r Number of Loans by Interest, echo=FALSE, cache=TRUE, fig.width=11, fig.height=7}
#creating new variables
summary(d$BorrowerRate)

ggplot(aes(x = BorrowerRate), data = d) +
  geom_histogram(binwidth = 0.005) + 
  ggtitle('Frequency of Interest Rate (bin width =  0.5%)')
```

Definitely there is something different after 30% of annual interest rate. Let's take a close lookat the range between 30%-37%.

```{r Number of Loans by Interest, echo=FALSE, cache=TRUE, fig.width=11, fig.height=7}
#creating new variables
ggplot(aes(x = BorrowerRate), data = d) +
  geom_histogram(binwidth = 0.001) + 
  coord_cartesian(xlim=c(0.3, 0.37))
  ggtitle('Frequency of Interest Rate (bin width =  0.5%)')
```

Now, more specifically in the range 31.5% and 32%. 

```{r, echo=FALSE, cache=TRUE, fig.width=14, fig.height=9}
table(subset(d, BorrowerRate>=0.316 & BorrowerRate<0.32)$BorrowerRate)
```

It is curious. Maybe they were all  made at the same day. 




```{r, echo=FALSE, cache=TRUE, fig.width=14, fig.height=9}
#creating new variables
ggplot(aes(x = LoanOriginationDate.year), data = d) +
  geom_histogram() + 
  ggtitle('Number of Loans by Year')
```






### Add a Scaling Layer
Notes:

```{r Add a Scaling Layer}
install.packages("gridExtra")#sthash.TtPNxnxv.dpuf
library(gridExtra)

summary(pf$friend_count)

summary(log10(pf$friend_count + 1))


summary(sqrt(pf$friend_count))


p1 <- qplot(x = friend_count, data = pf, 
           xlab = 'Number of friends',
           ylab = 'Number of users in sample',           
           binwidth = 10) +
  scale_x_continuous(limits = c(0, 1000),
                     breaks = seq(0, 1000, 50)) 

p2 <- qplot(x = friend_count, data = pf,
           xlab = 'log(Number of friends)',
           ylab = 'Number of users in sample') +
  scale_x_log10(limits = c(1, 1000),
                     breaks = seq(1, 1000, 50)) 


p3 <- qplot(x = friend_count, data = pf, 
           xlab = 'sqrt(Number of friends)',
           ylab = 'Number of users in sample') +
  scale_x_sqrt(limits = c(0, 1000),
                     breaks = seq(0, 1000, 50)) 

grid.arrange(p1, p2, p3, ncol=3)


p1 <- ggplot(aes(x = friend_count), data = pf) + geom_histogram()

p2 <- p1 + scale_x_log10()

p3 <- p1 + scale_x_sqrt()



grid.arrange(p1, p2, p3, ncol=1)

```

***


### Frequency Polygons

```{r Frequency Polygons}
ggplot(aes(x = friend_count, y = ..count../sum(..count..)), data = subset(pf, !is.na(gender))) +
  geom_freqpoly(aes(color = gender)) + 
  scale_x_continuous(limits = c(0, 1000), breaks = seq(0, 1000, 50)) +
  xlab('Friend Count') + 
  ylab('Percentage of users with that friend count')

```

***

### Likes on the Web
Notes:

```{r Likes on the Web}

summary(pf$www_likes)


ggplot(aes(x = www_likes), data = subset(pf, !is.na(gender))) +
  geom_freqpoly(aes(color = gender)) + 
  scale_x_log10()

by(pf$www_likes, pf$gender, sum)
```


***

### Box Plots
Notes:

```{r Box Plots}
qplot(x = gender, y = friend_count,
      data = subset(pf, !is.na(gender)),
      geom = 'boxplot') 
```

#### Adjust the code to focus on users who have friend counts between 0 and 1000.

```{r}
qplot(x = gender, y = friend_count,
      data = subset(pf, !is.na(gender)),
      geom = 'boxplot') +
  scale_y_continuous(limits = c(0, 1000))
```

***

### Box Plots, Quartiles, and Friendships
Notes:

```{r Box Plots, Quartiles, and Friendships}
qplot(x = gender, y = friendships_initiated,
      data = subset(pf, !is.na(gender)),
      geom = 'boxplot') +
  scale_y_continuous(limits = c(0, 250))

by(pf$friendships_initiated, pf$gender, sum)

by(pf$friendships_initiated, pf$gender, summary)
```

#### On average, who initiated more friendships in our sample: men or women?
Response:
#### Write about some ways that you can verify your answer.
Response:
```{r Friend Requests by Gender}

```

Response:

***

### Getting Logical
Notes:

```{r Getting Logical}
summary(pf$mobile_likes)

summary(pf$mobile_likes > 0)

pf$mobile_check_in <- NA
pf$mobile_check_in <- ifelse(pf$mobile_likes>0, 1, 0)
pf$mobile_check_in <- factor(pf$mobile_check_in )
my_summ <- summary(pf$mobile_check_in)
my_summ[2]/(my_summ[1]+my_summ[2])

sum(pf$mobile_check_in==1)/length(pf$mobile_check_in)
```

Response:

***

### Analyzing One Variable
Reflection:

-Observing the values that your variable take, like missing values, distribution and outliers;
-Boxplot, frequency polygons and histograns are the basic and most important tools to undertand individual variable;
-We use transformarmations like log, square and booleans to uncover hidden patterns in the variables;



Scales and Multiple Histograms

``` {r,echo=FALSE}

qplot(x = price, data = diamonds) + facet_wrap(~cut, scale="free_y")
?facet_wrap

```



###Interquartile Range - IQR

```{r,echo=FALSE}

#find out the best and worst color
?diamonds

#Price range for the middle 50% of diamonds with color D (best)
x <- subset(diamonds, color == 'D')$price 
IQR(x) 
summary(x)

#Price range for the middle 50% of diamonds with color J (worst)
x <- subset(diamonds, color == 'J')$price 
IQR(x) 
summary(x)



```









<h3>Bivariate Section</h3>

-Relationship observed
-What are the strongest relationship found


### Scatterplot Matrix
Notes:
```{r Looking at Sample of Households}

install.packages('GGally')

library(GGally)

set.seed(1836)
pf_subset <- pf[,c(2:15)]
names(pf_subset)
ggpairs(pf_subset[sample.int(nrow(pf_subset), 1000), ])


```





### Alpha and Jitter
Notes:


It is curious to notice that the distribution to friends_count and friend 
initiated are pretty the same. Maybe the most of users are relatively new on the
website.

```{r Alpha and Jitter}
# Examine the relationship between friendships_initiated (y) and age (x)
# using the ggplot syntax.

# We recommend creating a basic scatter plot first to see what the distribution 
#looks like and then adjusting it by adding one layer at a time.

# What are your observations about your final plot?

# Remember to make adjustments to the breaks of the x-axis and to use apply 
# alpha and jitter.
names(pf)

p1 <- ggplot(aes(x = age, y = friend_count), data = pf) + 
  geom_point(alpha = 1/20, position = position_jitter( h = 0)) + 
  coord_trans(y = "sqrt") +
  xlim(13, 90)

p2 <- ggplot(aes(x = age, y = friendships_initiated), data = pf) + 
  geom_point(alpha = 1/20, position = position_jitter( h = 0)) + 
  coord_trans(y = "sqrt") +
  xlim(13, 90)


grid.arrange(p1, p2, ncol=1)


```




### Conditional Means
Notes:

```{r Conditional Means}
#grouping data
age_groups <- group_by(pf, age)
pf.fc_by_age <- summarise(age_groups,
          friend_count_mean = mean(friend_count),
          fries_count_median = median(friend_count),
          n = n()
          )
head(pf.fc_by_age)
#as my dataframe is not arranged...
pf.fc_by_age <- arrange(pf.fc_by_age, age)
head(pf.fc_by_age)


#doing the same

pf.fc_by_age <- pf %>%#channing function
  group_by(age) %>%
  summarise(friend_count_mean = mean(friend_count),
          fries_count_median = median(friend_count),
          n = n()) %>%
          arrange(age)
head(pf.fc_by_age)

```

### Overlaying Summaries with Raw Data
Notes:

- People with more than one tawsand friends are quite rare (90% are bellow this value)
- 25 to 60 are far bellow 500;

```{r Overlaying Summaries with Raw Data}
ggplot(aes(x = age, y = friend_count), data = pf) +
  coord_cartesian(xlim = c(13, 90), 
                  ylim = c(0,2000)) +
  geom_point(alpha = 0.05,
            position = position_jitter(h=0),
            color = 'orange') + 
  geom_line(stat = 'summary', fun.y = mean) +
  geom_line(stat = 'summary', fun.y = quantile, prob = .1,
            linetype = 2, color = "blue") + 
  geom_line(stat = 'summary', fun.y = quantile, prob = .9,
            linetype = 2, color = "blue")



```


### Strong Correlations
Notes:

```{r Strong Correlations}
ggplot(aes(x = www_likes_received, y = likes_received), data = pf) +
  geom_point() +
  xlim (0, quantile(pf$www_likes_received, 0.95)) +
  ylim(0,quantile(pf$likes_received, 0.95)) +  
  geom_smooth(method = 'lm', color =  'red')
```

What's the correlation betwen the two variables? Include the top 5% of values for the variable in the calculation and round to 3 decimal places.

```{r Correlation Calcuation}
with(pf, cor.test(www_likes_received, likes_received))
```



### Making Sense of Data
Notes:

...it is sinusoidal... actually it makes sense because the seasons.... but it was cool to se in the chart when scratched

```{r Making Sense of Data}

range(Mitchell$Month)

ggplot(aes(y = Temp, x = Month), data = Mitchell) +
  geom_point() +
  scale_x_discrete(breaks = seq(0, 203,12))


ggplot(aes(x=(Month%%12),y=Temp),data=Mitchell)+ 
  geom_point() 
```

### Noise in Conditional Means

```{r Noise in Conditional Means}
p1 <- ggplot(aes(x = age_with_months, y = friend_count_mean), 
             data = subset(pf.fc_by_age_months, age_with_months < 71)) + 
  geom_line() +
  geom_smooth()


p2 <- ggplot(aes(x = age, y = friend_count_mean), 
             data = subset(pf.fc_by_age, age < 71)) + 
  geom_line() +
  geom_smooth()


p3 <- ggplot(aes(x = round(age/5)*5, y = friend_count_mean), 
             data = subset(pf.fc_by_age, age < 71)) + 
  geom_line()

library(gridExtra)
grid.arrange(p3, p2,p1,ncol=1)

```


###  create a new data frame containing info on diamonds by clarity
Notes:

...


```{r, echo=FALSE}

#grouping data
diamonds_groups <- group_by(diamonds, clarity)
diamondsByClarity <- summarise(diamonds_groups,
          mean_price = mean(price),
          median_price = median(as.numeric(price)),
          min_price = min(price),
          max_price = max(price),
          n = n()
          )
head(diamondsByClarity)

#as my dataframe is not arranged...
pf.fc_by_age <- arrange(pf.fc_by_age, age)
head(pf.fc_by_age)

```

***



###  Create a bar plot of dplying data
Notes:


first...for some crazy reason the best color and best clarity (D and IF) are not the greatest price either.


We think something odd is going here. These trends seem to go against our intuition.

Mean price tends to decrease as clarity improves. The same can be said for color.

We encourage you to look into the mean price across cut.

```{r, echo=FALSE}


?diamonds

diamonds_by_clarity <- group_by(diamonds, clarity)
diamonds_mp_by_clarity <- summarise(diamonds_by_clarity, mean_price = mean(price))

diamonds_by_color <- group_by(diamonds, color)
diamonds_mp_by_color <- summarise(diamonds_by_color, mean_price = mean(price))

diamonds_by_cut <- group_by(diamonds, cut)
diamonds_mp_by_cut <- summarise(diamonds_by_cut, mean_price = mean(price))

str(diamonds_mp_by_clarity)

p1 <- ggplot(aes(x = clarity, y= mean_price), data = diamonds_mp_by_clarity) + 
  geom_bar(stat="identity")
  
p2 <- ggplot(aes(x = color, y= mean_price), data = diamonds_mp_by_color) + 
  geom_bar(stat="identity") 

p3 <- ggplot(aes(x =cut, y= mean_price), data = diamonds_mp_by_cut) + 
  geom_bar(stat="identity") 

grid.arrange(p1, p2, p3, ncol = 1)
```


### Analyzing Two Variables
Reflection:

-sccater plots and conditional means;
-the bennefits and limitations of use of correlation and how correlation can affect
your decision on including the variables in you final model (do not use variables
without monotonic behaviour...that influence each other);
-we learned how to make sense of our data adjusting our visualizations...we learned
how jitter and alpha reduce overplotting;






<h3>Multivariate Section</h3>

-Relationship observed
-Any surprise?




### Histograms Revisited
Notes:

One clue to the discritness of data is that the 3rd percentile is the same of the maximum value

```{r Histograms Revisited}

yo <- read.csv('yogurt.csv')
str(yo)

#change the id from an int to a factor
yo$id <- factor(yo$id)
str(yo)

ggplot(data = yo, aes(x = price)) +
  geom_histogram()

ggplot(data = yo, aes(x = price)) +
  geom_histogram(binwidth=10)


summary(yo$price)

unique(yo$price)

length(unique(yo$price))

```

```{r, echo=FALSE}

sample_data <- function(sample_size = 3000){
  index <- sample(nrow(d), sample_size)
  return(d[index, ]) 
}

head(sample_data())

ggplot(aes(x = color, y = price), data = sample_data(sample_size = 5000)) +
  scale_color_brewer(type = 'div') +
  geom_boxplot(outlier.size = 0, color = "blue") +
  geom_jitter(position=position_jitter(width=0.3), alpha=0.2, 
              color = "steelblue2") +
  facet_wrap (~clarity)
```




<h2>Final Plots and Summary</h2>

- plot1 and description
- plot2 and description
- plot3 and description


<h2>Reflection</h2>

bla
